{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-DotPZn3nJVU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 1 (주관식 - 개념 설명)\n",
        "- 경사 하강법(Gradient Descent)의 목표와 핵심 원리를 \"손실(Loss)\"과 \"경사(Gradient)\"라는 키워드를 사용하여 2~3줄로 간략히 설명하시오"
      ],
      "metadata": {
        "id": "REx3fnH2nZao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "답안을 작성하세요.\n",
        "- 경사 하강법은 손실(Loss) 을 최소화하기 위해\n",
        "손실 함수에 대한 경사(Gradient) 를 계산하고,\n",
        "경사가 가리키는 방향의 반대로 weight와 bias를 반복적으로 업데이트하는 최적화 방법이다.\n"
      ],
      "metadata": {
        "id": "r0iPgNT8nrrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 2 (실습 문제 - 코드 작성)\n",
        "\n",
        "평균 제곱 오차(MSE: Mean Squared Error) 손실 함수를 파이토치 텐서를 사용하여 직접 구현하시오.\n",
        "\n",
        "- 함수 mse(Yp, Y)는 예측값 텐서 Yp와 실제값 텐서 Y를 입력받아 MSE 손실 값을 계산하여 반환해야 합니다.\n",
        "\n",
        "- MSE 공식: $\\text{loss} = \\frac{1}{n} \\sum_{i=1}^{n} (Yp_i - Y_i)^2$\n"
      ],
      "metadata": {
        "id": "LpaE01Y7nehv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 아래 함수를 완성하시오\n",
        "def mse(Yp, Y):\n",
        "    # Yp: 예측값 텐서, Y: 실제값 텐서\n",
        "    loss = ((Yp-Y)**2).mean()\n",
        "    return loss\n",
        "\n",
        "# --- 테스트 코드 (수정 불필요) ---\n",
        "Yp_test = torch.tensor([1.0, 2.5, 3.8])\n",
        "Y_test  = torch.tensor([1.2, 2.0, 4.0])\n",
        "# 예상 MSE = ((1.0-1.2)^2 + (2.5-2.0)^2 + (3.8-4.0)^2) / 3 = (0.04 + 0.25 + 0.04) / 3 = 0.33 / 3 = 0.11\n",
        "test_loss = mse(Yp_test, Y_test)\n",
        "print(f\"테스트 MSE 손실: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "kpDT0Xb9nWSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "305c6577-59de-4526-ce0d-abe5174c6050"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 MSE 손실: 0.1100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 3 (실습 문제 - 코드 빈칸 채우기)\n",
        "\n",
        "아래 코드의 빈칸 ( # TODO: ... 부분)을 채워 파라미터 W와 B를 업데이트하는 과정을 완성하시오.\n",
        "\n",
        "요구사항\n",
        "\n",
        "1. loss.backward()를 호출하여 경사를 계산합니다.\n",
        "2. torch.no_grad() 컨텍스트 내에서 W와 B를 학습률(lr)과 계산된 경사(.grad)를 이용하여 업데이트합니다.\n",
        "3. 다음 반복을 위해 W와 B의 경사 값을 0으로 초기화합니다 (.grad.zero_()).\n",
        "\n",
        "- 참고 : first_ml.ipynb)의 경사 하강법 반복 학습 부분을 참고"
      ],
      "metadata": {
        "id": "iMr1g2wToDrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np # 예시 데이터 생성을 위해 사용\n",
        "\n",
        "# 예시 데이터 (수정 불필요)\n",
        "X = torch.tensor([-5.,  5.,  0.,  2., -2.]).float()\n",
        "Y = torch.tensor([-6.7, 10.3, -3.3, 5.0, -5.3]).float()\n",
        "\n",
        "# 초기 파라미터 및 학습률 (수정 불필요)\n",
        "W = torch.tensor(1.0, requires_grad=True).float()\n",
        "B = torch.tensor(1.0, requires_grad=True).float()\n",
        "lr = 0.001\n",
        "\n",
        "# 예측 함수 및 손실 함수 (수정 불필요)\n",
        "def pred(X): return W * X + B\n",
        "def mse(Yp, Y): return ((Yp - Y)**2).mean()\n",
        "\n",
        "# --- 1회 반복 학습 과정 ---\n",
        "# 예측 계산 (수정 불필요)\n",
        "Yp = pred(X)\n",
        "\n",
        "# 손실 계산 (수정 불필요)\n",
        "loss = mse(Yp, Y)\n",
        "\n",
        "# TODO: 1. 경사 계산\n",
        "loss.backward()\n",
        "\n",
        "# 경사 업데이트 (torch.no_grad() 사용)\n",
        "with torch.no_grad():\n",
        "    W -= lr * W.grad\n",
        "    B -= lr * B.grad\n",
        "\n",
        "W.grad.zero_()\n",
        "B.grad.zero_()\n",
        "\n",
        "\n",
        "# --- 결과 확인 (수정 불필요) ---\n",
        "print(f\"업데이트 후 W: {W.item():.4f}\") # 초기 W=1.0, 초기 loss=13.3520, W.grad=-19.04, lr=0.001 -> 1.0 - 0.001*(-19.04) = 1.01904\n",
        "print(f\"업데이트 후 B: {B.item():.4f}\") # 초기 B=1.0, B.grad=2.0, lr=0.001 -> 1.0 - 0.001*(2.0) = 0.998\n",
        "print(f\"W의 현재 경사: {W.grad}\") # 초기화 후에는 0 또는 None 이어야 함\n",
        "print(f\"B의 현재 경사: {B.grad}\") # 초기화 후에는 0 또는 None 이어야 함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oWowWM-oOSD",
        "outputId": "a64b11f7-d40c-4307-a008-07233fabc001"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "업데이트 후 W: 1.0190\n",
            "업데이트 후 B: 0.9980\n",
            "W의 현재 경사: 0.0\n",
            "B의 현재 경사: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 4 (실습 문제 - 코드 빈칸 채우기)\n",
        "- torch.optim 라이브러리를 사용하여 확률적 경사 하강법(SGD) 옵티마이저를 생성하고, 이를 이용해 파라미터를 업데이트하는 코드의 빈칸을 채우시오\n",
        "\n",
        "요구사항\n",
        "1. optim.SGD를 사용하여 W와 B를 업데이트하는 옵티마이저(optimizer)를 생성합니다. 학습률(lr)도 지정해야 합니다.\n",
        "2. 계산된 경사를 이용하여 파라미터를 업데이트하기 위해 optimizer.step()를 호출합니다.\n",
        "3. 다음 반복을 위해 옵티마이저에 연결된 파라미터들의 경사도를 0으로 초기화하기 위해 optimizer.zero_grad()를 호출합니다."
      ],
      "metadata": {
        "id": "9FoboM3hpPcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np # 예시 데이터 생성을 위해 사용\n",
        "\n",
        "# 예시 데이터 (수정 불필요)\n",
        "X = torch.tensor([-5.,  5.,  0.,  2., -2.]).float()\n",
        "Y = torch.tensor([-6.7, 10.3, -3.3, 5.0, -5.3]).float()\n",
        "\n",
        "# 초기 파라미터 및 학습률 (수정 불필요)\n",
        "W = torch.tensor(1.0, requires_grad=True).float()\n",
        "B = torch.tensor(1.0, requires_grad=True).float()\n",
        "lr = 0.001\n",
        "\n",
        "# 예측 함수 및 손실 함수 (수정 불필요)\n",
        "def pred(X): return W * X + B\n",
        "def mse(Yp, Y): return ((Yp - Y)**2).mean()\n",
        "\n",
        "# TODO: 1. SGD 옵티마이저 생성 (파라미터: [W, B], 학습률: lr)\n",
        "optimizer = optim.SGD([W,B], lr=lr)\n",
        "\n",
        "# --- 1회 반복 학습 과정 ---\n",
        "# 예측 계산 (수정 불필요)\n",
        "Yp = pred(X)\n",
        "\n",
        "# 손실 계산 (수정 불필요)\n",
        "loss = mse(Yp, Y)\n",
        "\n",
        "# 경사 계산 (수정 불필요)\n",
        "loss.backward()\n",
        "\n",
        "# TODO: 2. 옵티마이저를 이용한 파라미터 업데이트\n",
        "optimizer.step()\n",
        "\n",
        "# TODO: 3. 옵티마이저 경사 초기화\n",
        "optimizer.zero_grad()\n",
        "\n",
        "\n",
        "# --- 결과 확인 (수정 불필요) ---\n",
        "# 결과는 문제 3과 동일해야 함\n",
        "print(f\"옵티마이저 사용 후 W: {W.item():.4f}\")\n",
        "print(f\"옵티마이저 사용 후 B: {B.item():.4f}\")\n",
        "print(f\"W의 현재 경사: {W.grad}\")\n",
        "print(f\"B의 현재 경사: {B.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7_68yY4n_ee",
        "outputId": "d4c94429-044c-4ff8-82c5-319b4229a9d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "옵티마이저 사용 후 W: 1.0190\n",
            "옵티마이저 사용 후 B: 0.9980\n",
            "W의 현재 경사: None\n",
            "B의 현재 경사: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 5(주관식-개념설명)\n",
        "\n",
        "모델 학습 시 발생할 수 있는 과소적합(Underfitting)과 과대적합(Overfitting)의 특징을 편향(Bias)과 분산(Variance) 관점에서 각각 설명하시오. (각각 1~2줄)"
      ],
      "metadata": {
        "id": "ENK7dByipgd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 과소적합 : 모델이 너무 단순하여 편향(Bias)이 크고 분산(Variance)은 작다.\n",
        "훈련 데이터조차 잘 설명하지 못해 훈련·검증 오차가 모두 크게 나타난다.\n",
        "- 과대적합 : 모델이 지나치게 복잡하여 편향(Bias)은 작고 분산(Variance)이 크다.\n",
        "훈련 데이터에는 잘 맞지만, 새로운 데이터에서는 검증 오차가 크게 증가한다."
      ],
      "metadata": {
        "id": "i8Rw3ACHprxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 6 (주관식 - 평가 지표 선택)\n",
        "- 다음과 같은 두 가지 상황에 가장 적합한 분류 모델 평가 지표를 각각 선택하고 그 이유를 간략히 설명하시오.\n",
        "\n",
        "- 상황 1: 환자의 의료 데이터를 분석하여 암 진단 여부를 예측하는 모델 (실제 암 환자를 놓치면 안 되는 경우, 즉 FN(False Negative)을 최소화해야 함)\n",
        "\n",
        "- 상황 2: 이메일 데이터를 분석하여 스팸 메일 여부를 필터링하는 모델 (정상 메일을 스팸으로 잘못 분류하면 안 되는 경우, 즉 FP(False Positive)를 최소화해야 함)\n",
        "\n",
        "- 선택 가능한 지표: 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1-Score"
      ],
      "metadata": {
        "id": "JE9Iio5_rqGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 상황1 : 재현율은 실제 양성(암 환자) 중에서 모델이 얼마나 많이 놓치지 않고 맞혔는지를 나타내는 지표이다.\n",
        "암 환자를 놓치는 False Negative(FN) 를 최소화해야 하므로 재현율이 가장 적합하다.\n",
        "- 상황2 : 정밀도는 모델이 스팸이라고 예측한 메일 중에서 실제로 스팸인 비율을 의미한다.\n",
        "정상 메일을 스팸으로 잘못 분류하는 False Positive(FP) 를 줄이기 위해 정밀도가 중요하다."
      ],
      "metadata": {
        "id": "2eqQ4XdWr1ol"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3EQpn5Ympdgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
